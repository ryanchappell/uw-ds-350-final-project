library(jsonlite)
library(logging)
library(plyr)
library(stringr)
library(data.table)
library(e1071)
library(caret)
library(rms)

source('helpers.R')

# log settings
#basicConfig("DEBUG")
basicConfig("INFO")
addHandler(writeToFile, file = "how-long-on-front-page-pcaic.log")

loginfo("Starting up!")

loginfo("Running unit tests")
getDateUtc_Test()
getTimestampFromFilename_Test()
addTimestamp_Test()
removeReviews_Test()
cleanAndStemText_Test()
removeReviews_Test()

# directory to read reddit results from 
allCsvDir = 'C:/reddit-csv/front every 30/'

# read entries
loginfo("Reading data")
all <- readAllCsvFilesInDirectory(allCsvDir)


# Attempt to predict how long a post would be on the front page.
allSample = data.table(all)
# set the index on the table on the id, so
# we can do sapply quickly further down
setkey(allSample,id)

loginfo("Aggregate total time of post on front page (this may take a while)")
aggSubTime = sapply(unique(allSample$id), FUN = function(postId){
  timestamps = allSample[paste0(postId),]$timestamp

  if (length(timestamps) <= 1){
    return(60) # no other timestamps, just return 1 minute
  } else {
    result = sum(diff(timestamps))
    return(result / 1000)
  }
})

allSample = unique(allSample)
loginfo(paste0("Total unique posts: ", nrow(allSample)))  
allSample$total_time = aggSubTime

dateCreated = getDateUtc(allSample$created_utc * 1000) # adjust for millsecond timestamp

allSample$hour = as.numeric(format(dateCreated, "%H"))
allSample$DOW = as.numeric(format(dateCreated, "%u"))

whichPage <- "front"

minutesOnFrontPage = aggSubTime / 60

titleText = paste0("From ", nrow(allSample),
                   " posts, frequency of hours on\r\n", whichPage ," page",
                   " for each post from\r\n",
                   as.POSIXct(min(allSample$timestamp) / 1000, origin="1970-01-01"), 
                   " to ", 
                   as.POSIXct(max(allSample$timestamp) / 1000, origin="1970-01-01"))

# histogram of total hours on front page
hoursOnFrontPage = minutesOnFrontPage / 60
hist(hoursOnFrontPage, xlim = c(0,25), ylim = c(0,2000), 
     xlab = "hours on front page",
     breaks = 25, main = titleText)

modelFun = total_time ~ subreddit + over_18 + hour + is_self + DOW

lmAll = lm(modelFun, data = allSample)

loginfo("Create a data matrix")
allMatrix = model.matrix(modelFun, data = allSample)

train_ind = sample(1:nrow(allSample), round(0.8*nrow(allSample)))

loginfo("Get principal components")
pcAll = prcomp(allMatrix)

loginfo("Plot principle components")
plot(pcAll$sdev)

loginfo("Getting AIC per number of components")
numFeatures = dim(pcAll$x)[2]
allPcAic = sapply(2:numFeatures, function(x){
  formula_rhs_temp = paste(paste0('pcAll$x[,',1:x,']'), collapse = ' + ')
  formula_temp = paste('allSample$total_time ~',formula_rhs_temp)
  pc_all_components_temp = lm(eval(parse(text=formula_temp)))
  return(AIC(pc_all_components_temp))
})

loginfo("Plot AIC per number of components along with full linear regression line")
plot(allPcAic, type='l', lwd=2,
     main='AIC of P.C. Linear Reg with X components',
     xlab="# of components", ylab='AIC')
# add a horizontal line of where the all variable AIC is at
abline(h=AIC(lmAll), lwd=2, col='red')

minPcAic = which.min(allPcAic)

# get model for minimum principal components
minPcAicLm = lm(allSample$total_time ~ pcAll$x[,1:minPcAic])

# get adjusted R-squared values for comparison
minPcAicLmAdjustedRSquared = summary(minPcAicLm)$adj.r.squared
fullLmAdjustedRSquared = summary(lmAll)$adj.r.squared

loginfo(paste0("#### Summary:\r\n",
               "## To get a good fit with SVD, we looked at a linear regression ",
               "of the AIC for 2 to n number of features to find the number ",
               "of features (or, principal components) with the lowest AIC. ",
               "The point at which the AIC ",
               "is minimum is ", minPcAic, " features.\r\n",
               "## To compare the variance explained by the full model ",
               "and the model generated by finding the SVD model with the minimum ",
               "AIC, we can look at the adjusted R-squared. ",
               "The adjusted R-squared for the full crime model is ", fullLmAdjustedRSquared,
               ". The adjusted R-squared for the SVD model with minimum AIC is ", minPcAicLmAdjustedRSquared,
               "."))

train_ind = sample(1:nrow(allSample), round(0.8*nrow(allSample)))
train_set = allSample[train_ind,]
test_set = allSample[-train_ind,]

reviewStylePredictions = predict(lmAll, newdata = test_set, interval = "prediction")

# THE PERFORMANCE OF THIS IS HORRIBLE :(
plot(reviewStylePredictions[,c("fit")] / 60 / 60, test_set$total_time / 60 / 60,
     main = "Predicted vs. actual in hours", xlim = c(0,24)) # predicted vs actual in hours
hist(setdiff(reviewStylePredictions[,1], test_set$total_time) / 60 / 60, xlim = c(0,20),
     main = "Difference in predicted and actual in hours") # difference of predictions in hours

